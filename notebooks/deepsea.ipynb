{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install jax jaxlib dm-env dm-haiku bsuite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque, namedtuple\n",
    "from copy import deepcopy\n",
    "from functools import partial\n",
    "from typing import Callable, List, NamedTuple, Optional, Tuple, Union\n",
    "\n",
    "import dm_env\n",
    "import haiku as hk\n",
    "import jax\n",
    "import numpy as np\n",
    "from bsuite.baselines.base import Agent\n",
    "from bsuite.environments.base import Environment\n",
    "from dm_env import specs\n",
    "from jax import numpy as jnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Observation = np.ndarray\n",
    "Action = int\n",
    "Features = Union[np.ndarray, jnp.ndarray]\n",
    "BasisFunction = Callable[[Observation, Action], Features]\n",
    "AgentState = namedtuple('AgentState', \"seq_weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepSea(Environment):\n",
    "\n",
    "    def __init__(self, size: int):\n",
    "        super().__init__()\n",
    "        self._size = size\n",
    "        self._t = 0\n",
    "        self._total_cost = 0\n",
    "        self._seq_cost = []\n",
    "        self._column = 0\n",
    "        self._row = 0\n",
    "        self._reset()\n",
    "\n",
    "    def _get_observation(self):\n",
    "        obs = np.zeros(shape=(2 * self._size,), dtype=np.float32)\n",
    "        obs[self._row] = 1.\n",
    "        obs[self._size + self._column] = 1.\n",
    "        return obs\n",
    "\n",
    "    def _reset(self) -> dm_env.TimeStep:\n",
    "        self._row = 0\n",
    "        self._column = 0\n",
    "        self._timestep = 0\n",
    "        return dm_env.restart(self._get_observation())\n",
    "\n",
    "    def _step(self, action: int) -> dm_env.TimeStep:\n",
    "\n",
    "        self._row = (self._row + 1) % self._size\n",
    "        self._column = (self._column + 2 * action - 1) % self._size\n",
    "\n",
    "        if (self._row == self._size - 1) and (self._column == self._size - 1):\n",
    "            reward = 2 * self._size\n",
    "        else:\n",
    "            reward = -float(action)\n",
    "\n",
    "        self._t += 1\n",
    "        self._total_cost -= reward\n",
    "        self._seq_cost.append(self._avg_cost)\n",
    "\n",
    "        observation = self._get_observation()\n",
    "\n",
    "        return dm_env.transition(reward=reward, observation=observation)\n",
    "\n",
    "    def observation_spec(self):\n",
    "        return specs.Array(shape=(2 * self._size,), dtype=np.float32)\n",
    "\n",
    "    def action_spec(self):\n",
    "        return specs.DiscreteArray(2, name='action')\n",
    "\n",
    "    @property\n",
    "    def _avg_cost(self):\n",
    "        return round(self._total_cost / self._t, 2)\n",
    "\n",
    "    def bsuite_info(self):\n",
    "        return dict(avg_cost=self._avg_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basis function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_basis(size: int) -> BasisFunction:\n",
    "\n",
    "    nS, nA = size, 2\n",
    "\n",
    "    # basis function\n",
    "    def basis_function(\n",
    "        o: Observation,\n",
    "        a: Action,\n",
    "    ) -> Features:\n",
    "        f = jnp.zeros((nA, 2 * nS))\n",
    "        f = jax.ops.index_update(f, jax.ops.index[a], o)\n",
    "        return f.flatten()\n",
    "\n",
    "    return basis_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive Approximate Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def _eval(\n",
    "    agent_state: AgentState,\n",
    "    batch_f: jnp.ndarray,\n",
    "    batch_r: jnp.ndarray,\n",
    ") -> AgentState:\n",
    "    batch_rev_q = jnp.cumsum(batch_r[::-1]) / jnp.arange(1, 1 + len(batch_r))\n",
    "    batch_q = batch_rev_q[::-1]\n",
    "    weights = jnp.linalg.lstsq(batch_f, batch_q)[0]\n",
    "    return agent_state._replace(seq_weights=agent_state.seq_weights + [weights])\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnums=(4,))\n",
    "def _select_last(\n",
    "    agent_state: jnp.ndarray,\n",
    "    batch_f: jnp.ndarray,\n",
    "    lr: float,\n",
    "    key: np.ndarray,\n",
    "    size: Optional[int] = 30,\n",
    ") -> Action:\n",
    "    w_all = agent_state.seq_weights\n",
    "    if size is not None:\n",
    "        w_all = w_all[-(size + 1):]\n",
    "    if len(w_all) == 1:\n",
    "        q = jnp.matmul(batch_f, w_all[0])\n",
    "        div = jnp.linalg.norm(q - jnp.mean(q), ord='inf')\n",
    "        logits = 2 * q\n",
    "        temperature = lr * jnp.sqrt(2) * div\n",
    "    else:\n",
    "        q_all = jnp.matmul(jnp.stack(w_all), batch_f.T)\n",
    "        div = jnp.linalg.norm(q_all[1:] - q_all[:-1], ord='inf', axis=1)\n",
    "        logits = q_all[1:].sum(axis=0) + q_all[-1]\n",
    "        temperature = lr * jnp.sqrt(2 * jnp.power(div, 2).sum())\n",
    "    action = jax.random.categorical(key, logits / temperature)\n",
    "    return action\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnums=(4,))\n",
    "def _select_random(\n",
    "    agent_state: jnp.ndarray,\n",
    "    batch_f: jnp.ndarray,\n",
    "    lr: float,\n",
    "    key: np.ndarray,\n",
    "    size: Optional[int] = 5,\n",
    ") -> Action:\n",
    "    w_all = agent_state.seq_weights\n",
    "    q = jnp.matmul(batch_f, w_all[-1])\n",
    "    if (size is not None) and len(w_all) > size:\n",
    "        idxs = jax.random.choice(key,\n",
    "                                 jnp.arange(1, len(w_all)), (size,),\n",
    "                                 replace=False)\n",
    "        scale = jnp.sqrt(len(w_all) / size)\n",
    "    else:\n",
    "        idxs = jnp.arange(1, len(w_all))\n",
    "        scale = 1\n",
    "    if len(w_all) == 1:\n",
    "        div = jnp.linalg.norm(q - jnp.mean(q), ord='inf')\n",
    "        logits = 2 * q\n",
    "        temperature = lr * jnp.sqrt(2) * div\n",
    "    else:\n",
    "        w = jnp.stack(w_all)[idxs]\n",
    "        w_ = jnp.stack(w_all)[idxs - 1]\n",
    "        q_all = jnp.matmul(jnp.stack(w), batch_f.T)\n",
    "        q_all_ = jnp.matmul(jnp.stack(w_), batch_f.T)\n",
    "        div = jnp.linalg.norm(q_all - q_all_, ord='inf', axis=1)\n",
    "        logits = q_all.sum(axis=0) + q\n",
    "        temperature = lr * jnp.sqrt(2 * jnp.power(div, 2).sum())\n",
    "        temperature *= scale\n",
    "    action = jax.random.categorical(key, logits / temperature)\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Buffer:\n",
    "\n",
    "    def __init__(self, capacity: Optional[int] = None):\n",
    "\n",
    "        self._observations = deque(maxlen=capacity)\n",
    "        self._actions = deque(maxlen=capacity)\n",
    "        self._rewards = deque(maxlen=capacity)\n",
    "\n",
    "    def append(self, timestep: dm_env.TimeStep, action: Action,\n",
    "               new_timestep: dm_env.TimeStep):\n",
    "\n",
    "        self._observations.append(timestep.observation)\n",
    "        self._actions.append(action)\n",
    "        self._rewards.append(new_timestep.reward)\n",
    "\n",
    "        if new_timestep.last():\n",
    "            raise AssertionError(\n",
    "                'Environment should have infinite horizon (see paper).')\n",
    "\n",
    "    def get(self) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        batch_o = np.stack(self._observations)\n",
    "        batch_a = np.stack(self._actions)\n",
    "        batch_r = np.stack(self._rewards)\n",
    "        return batch_o, batch_a, batch_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AAPI(Agent):\n",
    "\n",
    "    def __init__(self,\n",
    "                 obs_spec: specs.Array,\n",
    "                 action_spec: specs.DiscreteArray,\n",
    "                 basis_function: BasisFunction,\n",
    "                 train_every: int,\n",
    "                 learning_rate: float,\n",
    "                 buffer_size: int,\n",
    "                 rng: hk.PRNGSequence,\n",
    "                 weight_select: str = 'last',\n",
    "                 weight_size: Optional[int] = None):\n",
    "        assert weight_select in [\n",
    "            'random', 'last'\n",
    "        ], \"'weight_select' take values in ['random', 'last'] \"\n",
    "        self._obs_spec = obs_spec\n",
    "        self._action_spec = action_spec\n",
    "        self._basis_function = jax.vmap(basis_function)\n",
    "        self._train_every = train_every\n",
    "        self._learning_rate = learning_rate\n",
    "        self._rng = rng\n",
    "        self._weight_size = weight_size\n",
    "\n",
    "        if weight_select == 'random':\n",
    "            self._select = _select_random\n",
    "        else:\n",
    "            self._select = _select_last\n",
    "\n",
    "        # initialize buffer\n",
    "        capacity = buffer_size if buffer_size > 0 else None\n",
    "        self._t = 0\n",
    "        self._buffer = Buffer(capacity)\n",
    "\n",
    "        # initialize parameters\n",
    "        self._state = AgentState(seq_weights=[])\n",
    "\n",
    "    def update(\n",
    "        self,\n",
    "        timestep: dm_env.TimeStep,\n",
    "        action: Action,\n",
    "        new_timestep: dm_env.TimeStep,\n",
    "    ):\n",
    "        self._t += 1\n",
    "        self._buffer.append(timestep, action, new_timestep)\n",
    "        if self._t % self._train_every == 0:\n",
    "            batch_o, batch_a, batch_r = self._buffer.get()\n",
    "            batch_f = self._basis_function(batch_o, batch_a)\n",
    "            self._state = _eval(self._state, batch_f, batch_r)\n",
    "\n",
    "    def select_action(\n",
    "        self,\n",
    "        timestep: dm_env.TimeStep,\n",
    "    ) -> Action:\n",
    "        nA = self._action_spec.num_values\n",
    "        if len(self._state.seq_weights) == 0:\n",
    "            action = jax.random.randint(next(self._rng), (), 0, nA)\n",
    "            return int(action)\n",
    "        lr = self._learning_rate\n",
    "        o = timestep.observation\n",
    "        batch_o = jnp.repeat(np.expand_dims(o, 0), nA, axis=0)\n",
    "        batch_a = jnp.arange(nA)\n",
    "        batch_f = self._basis_function(batch_o, batch_a)\n",
    "        action = self._select(self._state, batch_f, lr, next(self._rng),\n",
    "                              self._weight_size)\n",
    "        return int(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(agent: Agent,\n",
    "        environment: dm_env.Environment,\n",
    "        num_steps: int,\n",
    "        verbose: bool = False) -> None:\n",
    "\n",
    "    timestep = environment.reset()\n",
    "\n",
    "    for t in range(num_steps):\n",
    "        # Generate an action from the agent's policy.\n",
    "        action = agent.select_action(timestep)\n",
    "\n",
    "        # Step the environment.\n",
    "        new_timestep = environment.step(action)\n",
    "\n",
    "        # Tell the agent about what just happened.\n",
    "        agent.update(timestep, action, new_timestep)\n",
    "\n",
    "        # Book-keeping.\n",
    "        timestep = new_timestep\n",
    "\n",
    "        if verbose and (t % agent._train_every == 0):\n",
    "            bsuite_info = environment.bsuite_info()\n",
    "            logs = ['step = {}'.format(t)] + [\n",
    "                '{} = {}'.format(key, item)\n",
    "                for key, item in bsuite_info.items()\n",
    "            ]\n",
    "            print(' | '.join(logs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 123  # Random seed\n",
    "size = 20  # Size of chain experiment\n",
    "train_steps = 20000  # Number of train steps.\n",
    "train_every = 50  #Period of train steps\n",
    "learning_rate = .1  # Learning rate\n",
    "buffer_size = 500  # Set to -1 if arbitrary size\n",
    "weight_size = 30  # Number of weights to select\n",
    "weight_select = 'last'  # Weight selection method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(seed)\n",
    "rng = hk.PRNGSequence(key)\n",
    "\n",
    "# set environment\n",
    "env = DeepSea(size=size)\n",
    "\n",
    "# set basis function\n",
    "basis_function = get_basis(size)\n",
    "\n",
    "# set agent\n",
    "agent = AAPI(obs_spec=env.observation_spec(),\n",
    "             action_spec=env.action_spec(),\n",
    "             basis_function=basis_function,\n",
    "             train_every=train_every,\n",
    "             learning_rate=learning_rate,\n",
    "             buffer_size=buffer_size,\n",
    "             weight_size=weight_size,\n",
    "             weight_select=weight_select,\n",
    "             rng=rng)\n",
    "\n",
    "# run experiment\n",
    "run(agent=agent, environment=env, num_steps=train_steps, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "x = range(env._t)\n",
    "y = env._seq_cost\n",
    "\n",
    "ax.plot(x, y)\n",
    "\n",
    "ax.set_xlabel('Episodes')\n",
    "ax.set_ylabel(r'Cumulative cost $-\\frac{1}{T}\\sum r_t$')\n",
    "ax.set_title('Deepsea of size {}'.format(size))\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [
    "katqG1libWKj"
   ],
   "name": "deepsea.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
